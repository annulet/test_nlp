{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\tWhat are the main problems of modern NLP and NLU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problems of NLU:\n",
    "    \n",
    "    - relation extraction\n",
    "    - semantic parsing\n",
    "    - sentiment analysis\n",
    "    - summarization\n",
    "    - paraphrase and natural language inference\n",
    "    - question answering (QA)\n",
    "    \n",
    "The tasks of NLP:\n",
    "\n",
    "    - named entity recognition (NER)\n",
    "    - part-of-speech tagging (POS)\n",
    "    - syntactic parsing\n",
    "    - text categorization\n",
    "    - core reference resolution\n",
    "    - machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.\tWhich libraries would you pick to use for the following cases and why (all problems should be solved for the Russian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tSentiment analysis: sklearn (preprocessing), NLTK, Pymystem3, PyMorphy2, pigeon-jupyter (for annotations), Gensim, stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tMulti-label classification: sklearn (preprocessing), NLTK, Pymystem3, PyMorphy2, pigeon-jupyter (for annotations), Gensim, stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tDependency parsing: DeepPavlov, Allennlp, UDPipe (contains models consisting of a tokenizer, tagger, lemmatizer and dependency parser, all trained using the UD data, so it suits the most cases of NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tPOS-tagging: UDPipe, PyMorphy2, spaCy, NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tNER: AllenNLP, PyMorphy2, the TreeTagger, Natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also, frameworks such as Pytorch, Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.\tHow would you evaluate a classification model, which metrics would you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `Accuracy` measures the fraction of the total sample that is correctly identified. With imbalanced classes, it’s easy to get a high accuracy without actually making useful predictions. So, accuracy as an evaluation metrics makes sense only if the class labels are uniformly distributed.\n",
    "\n",
    "2. `Confusion matrix` is a performance measurement for a classification algorithm where output can be two or more classes. It is good technique to summarizing the performance of a classification algorithm.\n",
    "\n",
    "3. `Precision` is a measure of the ability of a classification model to identify only the relevant data points.\n",
    "\n",
    "4. `Recall` is a measure of the ability of a model to find all the relevant cases within a dataset.\n",
    "\n",
    "Depending on the task we can choose between Precision and Recall.\n",
    "\n",
    "5. `The precision-recall curve` shows the trade-off between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.\n",
    "\n",
    "6. `AUC-ROC` is another very useful metric for imbalanced classed.\n",
    "\n",
    "I would use all those metrices choosing one as the main metrics based on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.\tMain pipeline for the text pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main pipeline for the text preprocessing is practically the same whatever the task of NLP you deal with and is as follows:\n",
    "\n",
    "- Lowercase the text\n",
    "- Remove stopwords\n",
    "- Tokenize sentences or words (when the task is translation we deal with sentences)\n",
    "- Remove punctuation, numbers, etc. (if the task is not NER or translation or similar)\n",
    "- Lemmatization/Stemming \n",
    "- POS-tags\n",
    "- Vectorization\n",
    "\n",
    "Thus we extract the context-independent features.\n",
    "To extract the context-depended features we can use Bidirectional RNN (LSTM or GRU) or transformer encoder architecture - ELMo, ULMFit, BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.\tMicroservices or monoliths? Why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe a pool of microservices around a single, monolithic core application. It depends on the task and the team and everything..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.\tDescribe the hardest programming task you’ve been facing with. It’s not necessarily ML task, could be just a programming. Why this task was hard to accomplish? What was your solution for the task? Can you share a github project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always hard to do something new. Every time I face an unknown problem it seems hard to me, interesting and challenging, but completely insoluble at first. But gradually you solve it. Recently I've carried out a final project on 'Recommender systems' course. The hard thing was not only to overcome 25% `precision@5` but to write a custom `class` for recommendations. It took me a lot of time and efforts to use OOP in practice but it was a very useful experience. Here is my project on GitHub: https://github.com/annulet/Recsys_project/tree/master/rec_sys_final_project  \n",
    "\n",
    "Actually there was another difficult task to generate human faces using DCGAN. But the problem was with GoogleColab and its' limits in using GPU and the result wasn't very good. https://github.com/annulet/CNN_NVIDIA/blob/master/hw_8_Gegeration_GAN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.\tDid you work with VCS? Which one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, I use GitHub and some time ago I was engaged in a project where they used GitLab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.\tDid you work with Github Actions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really, but I've read about it. It's to automate all your software workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.\tHow familiar are you with Docker and other orchestration tools?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've completed a course on Linux where deployment of Docker containers was a part of the course.\n",
    "\n",
    "Unfortunately, I've never had to manage container orchestration and never had a chance to deal with cluster management tools. But of course, I've heard about Kubernetes, Docker Swarm, Mesos and Cloud Based Container Clustering Services such as Google Container Engine, AWS EKS Service, Amazon EC2 Container Service, Azure AKS Service etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.\tWhat is ed25519 and why is it concerning to be better than ecdsa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ed25519 is a public-key algorithm. It’s the EdDSA implementation using the Twisted Edwards curve. It’s using elliptic curve cryptography that offers a better security with faster performance compared to DSA or ECDSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.\tDo you have any experience in data mining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. I've completed a course  «Methods of collecting and processing data from the Internet» while studying in GeekUniversity on the faculty of AI. Here's the certificate:  https://geekbrains.ru/go/wqE4MO\n",
    "\n",
    "And some example: https://github.com/annulet/parsing/blob/master/home_work_5.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
