{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\tWhat are the main problems of modern NLP and NLU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem is that understanding and processing natural language isn’t as simple as providing a huge set of vocabulary and training your machine on it. Successful NLP must blend techniques from a range of fields: `language`, `linguistics`, `data science`, `computer science`, and more.\n",
    "\n",
    "Machines can find patterns in numbers and statistics, but understanding language takes a lot more: an understanding of a language’s syntax, the difference in how a language is spoken or written, pure context and definitions, shifting language patterns and ever-developing new definitions, picking up on subtleties like sarcasm which aren’t inherently readable from text, or understanding the true purpose or goal of a body of content.\n",
    "\n",
    "This is where NLP and NLU come in. Though the concepts might feel straightforward, getting into the nitty-gritty details and their applications, you may need a crash course in computer programming and linguistics to really understand how complicated these concepts can be.\n",
    "\n",
    "Therein lies some complexity, experts say. After all, language understanding is often broken down into three linguistic levels:\n",
    "\n",
    "- Syntax – understanding the grammar of the text\n",
    "- Semantics – understanding the meaning of the text\n",
    "- Pragmatics – understanding what the text is trying to achieve\n",
    "\n",
    "Language is hard enough for a person to learn – we don’t have a single way to ensure language acquisition. More complicated are the ways that languages are always shifting, adding and subtracting from a vast lexicon, incorporating ways that emails, texts, social media are used to affect language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.\tWhich libraries would you pick to use for the following cases and why (all problems should be solved for the Russian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tSentiment analysis: sklearn, NLTK, pigeon-jupyter (for annotations), Gensim, Word2Vec, Seq2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tMulti-label classification: sklearn, NLTK, pigeon-jupyter (for annotations), Gensim, Word2Vec,Seq2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tDependency parsing: UDPipe is a trainable pipeline for tokenization, tagging, lemmatization and dependency parsing of CoNLL-U files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tPOS-tagging: Universal Dependencies (UD), UDPipe, Pymystem3, PyMorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tNER: AllenNLP, Pymystem3, PyMorphy2, the TreeTagger, natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.\tHow would you evaluate a classification model, which metrics would you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common metric for classification is Accuracy. But it measures the fraction of the total sample that is correctly identified. So, it won't be very informative without Precision and Recall if classes are unbalanced. Precision measures from all the examples predicted as positive, how many are actually positive. Recall measures from all the actual positives, how many examples were correctly classified as positive. And depending of our task we chose between Precision and Recall. But F1 Score is the harmonic mean of the Precision and Recall and is also very useful when classes are unbalanced. That's why I will use all of those metrics, chosing one as the main metric based on the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.\tMain pipeline for the text pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main pipeline for the text preprocessing is practically the same whatever the task of NLP you deel with and is as follows:\n",
    "\n",
    "- Lowecase the text\n",
    "- Remove stopwords\n",
    "- Tokenize sentences or words (when the task is translation we deel with sentences)\n",
    "- Remove punctuation, numbers, etc. (if the task is not NER or translation or similar)\n",
    "- Lemmatization/Stemming \n",
    "- POS-tags\n",
    "- Vectorization\n",
    "\n",
    "Thus we extract the context-independent features.\n",
    "To extract the context-depended features we can use Bidirectional RNN (LSTM or GRU) or transformer encoder architecture - ELMo, ULMFit, BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.\tMicroservices or monoliths? Why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.\tDescribe the hardest programming task you’ve been facing with. It’s not necessarily ML task, could be just a programming. Why this task was hard to accomplish? What was your solution for the task? Can you share a github project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always hard to do something new. Every time I face an unknown problem it seems hard to me, interesting and challenging, but completely insoluble at first. But gradually you solve it. Recently I've carried out a final project on 'Recommender systems' course. The hard thing was not only to overcome 25% `precision@5` but to write a custom `class` for recommendations. It took me a lot of time and efforts to use OOP in practice but it was a very useful experience. Here is my project on GitHub: https://github.com/annulet/Recsys_project/tree/master/rec_sys_final_project  \n",
    "\n",
    "Actually there was another difficult task to generate human faces using DCGAN. But the problem was with GoogleColab and its' limits in using GPU and the result wasn't very good. https://github.com/annulet/CNN_NVIDIA/blob/master/hw_8_Gegeration_GAN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.\tDid you work with VCS? Which one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, I use GitHub and some time ago I was engaged in a project where they used GitLab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.\tDid you work with Github Actions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really, but I've read about it. It's to automate all your software workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.\tHow familiar are you with Docker and other orchestration tools?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've completed a course on Linux where deployment of Docker containers was a part of the course.\n",
    "\n",
    "Unfortunately I've never had to manage container orchestration and never hac a chance to deal with cluster management tools. But of course I've heard about Kubernetes, Docker Swarm, Mesos and Cloud Based Container Clusterin Services such as Google Container Engine, AWS EKS Service, Amazon EC2 Container Service, Azure AKS Service etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.\tWhat is ed25519 and why is it concerning to be better than ecdsa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.\tDo you have any experience in data mining?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. I've completed a course  «Methods of collecting and processing data from the Internet» while studying in GeekUniversity on the faculty of AI. Here's the certificate:  https://geekbrains.ru/go/wqE4MO\n",
    "\n",
    "And some example: https://github.com/annulet/parsing/blob/master/home_work_5.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
